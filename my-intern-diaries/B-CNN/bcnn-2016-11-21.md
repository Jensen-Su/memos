---
2016/11/21, Monday, Rainy
---
    By Jincheng Su @ Hikvision, mail: jcsu14@fudan.edu.cn

## Making `/path/to/bcnn/run_experiments.m` run

1. After fine-tuning is done by `run_experiments_bcnn_train.m`, a fine-tuned model named `final-model.mat` is generated under directory `/path/to/bcnn/data/checkgpu/cub-seed-01/fine-tuned-model`. `cp` the file to `/path/to/bcnn/data/ft-models/`. You may want to change its name to something like `cub-bcnn-mmft.mat`.

2. Adjust the path of `modela` of the struct `bcnnmmft.opts` in `run_experiments.m`:

	```[matlab]
	% /path/to/bcnn/run_experiments.m

	  bcnnmmft.opts = {...
		'type', 'bcnn', ...
		'modela', 'data/ft_models/bcnn-cub-mmft.mat', ...
		'layera', [],...
		'modelb', [], ...
		'layerb', []
		} ;
	```

3. Added it to the lists `setupNameList` and `encoderList`:

    setupNameList = {'bcnnmmft', 'bcnnvdmft', 'bcnnvdvdft'};   % list of models to train and test
    encoderList = {{bcnnmmft}, {bcnnvdmft}, {bcnnvdvdft}};

4. run the file.

    	> run_experiments

	Error:

		Error using matlabpool (line XXX)
		matlabpool has been removed.
		To query the size of an already started parallel pool, query the 'NumWorkers'
		property of the pool.
		To check if a pool is already started use 'isempty(gcp('nocreate'))'.

5. Open file `/path/to/bcnn/model_train.m`, uncommet the line `numWorkers = matlabpool('size') ;`, run `run_experiments.m` again.

6. Error again:
	```
    Task: 001: encoder: extract features: image 11774 of 11788
    Task: 001: encoder: extract features: image 11775 of 11788
    Task: 001: encoder: extract features: image 11776 of 11788

    ...

    Task: 001: encoder: extract features: image 62 of 11788
    Task: 001: encoder: extract features: image 63 of 11788
    Task: 001: encoder: extract features: image 64 of 11788

    236 code = cell(1, numel(imageIds)*ts) ;
    Error using cat
    Out of memory. Type HELP MEMORY for your options.

    Error in model_train>encoder_extract_for_images (line 248)
       code = cat(2, code{:}) ;

    Error in model_train (line 39)
          code = encoder_extract_for_images(encoder, imdb, imdb.images.id, 'dataAugmentation',
          opts.dataAugmentation, 'scale', opts.imgScale) ;

    Error in run_experiments (line 127)
            model_train(...
	```

7. Look into the source code:

	```[matlab]
	% /path/to/bcnn/model_train.m

	for b = numel(batches):-1:1
	  batchResults{b} = get_batch_results(imdb, imageIds, batches{b}, ...
							encoder, opts.maxNumLocalDescriptorsReturned, opts.dataAugmentation, opts.scale) ;
	end

	...

	code = cell(1, numel(imageIds)*ts) ;
	for b = 1:numel(batches)
	  m = numel(batches{b});
	  for j = 1:m
		  k = batches{b}(j) ;
		  for aa=1:ts
			code{(k-1)*ts+aa} = batchResults{b}.code{(j-1)*ts+aa};
		  end
	  end
	end
	clear batchResults;
	if opts.concatenateCode
	   code = cat(2, code{:}) ;
	end
	```

	There are several big variables:

		batchResults: 185 * 128 * 262144 * 'single'
		batches: 185 * 64
		code: 23576 * 262144 * 'single'

		185 * 128 * 262144 * 'single' = 23 GByte
		23576 * 262144 * 'single' = 23 GByte

	In the ubuntu terminal:
		$ free -h
					 total       used       free     shared    buffers     cached
		Mem:           31G        30G       569M        73M        91M       2.1G
		-/+ buffers/cache:        28G       2.7G
		Swap:         7.6G        79M       7.6G
	RAM is ran out!

8. How about `clear batchResults` before `cat`?

	```[matlab]
	% model_train.m: function encoder_extract_for_images

	clear batchResults;
	if opts.concatenateCode
	   code = cat(2, code{:}) ;
	end
	```
	Run again,
		$ free -h

					 total       used       free     shared    buffers     cached
		Mem:           31G        30G       569M        73M        91M       2.1G
		-/+ buffers/cache:        28G       2.7G
		Swap:         7.6G        79M       7.6G

	Not change a bit! Is that because of `code` and `batchResults` sharing the same data? Do them really share the same data? But why doesn't `clear batchResults` affect `code`?'

9. It seems the only way is not to `cat`. Will it hurt if not `cat`? Just give it a shot and see!

	...

					 total       used       free     shared    buffers     cached
		Mem:           31G        31G       235M        73M        78M       2.3G
		-/+ buffers/cache:        28G       2.7G
		Swap:         7.6G        97M       7.5G

	After some minutes:
					 total       used       free     shared    buffers     cached
		Mem:           31G        31G       284M        73M       7.3M       2.4G
		-/+ buffers/cache:        28G       2.6G
		Swap:         7.6G       144M       7.5G

	It seems to begin using swap area.
	Afer some more minutes...
	Error!

		Task: 001: encoder: extract features: image 64 of 11788
		Warning: repmat(A,M) or repmat(A,M,N) where M or N is an empty array will return an error in a future release. Replace empty array inputs with 1 instead.
		> In model_train>traintest (line 106)
		  In model_train (line 55)
		  In run_experiments (line 127)
		Warning: repmat(A,M) or repmat(A,M,N) where M or N is an empty array will return an error in a future release. Replace empty array inputs with 1 instead.
		> In model_train>traintest (line 109)
		  In model_train (line 55)
		  In run_experiments (line 127)

		-------------------------------------- OVA-classifier: class: 1
		Warning: repmat(A,M) or repmat(A,M,N) where M or N is an empty array will return an error in a future release. Replace empty array inputs with 1 instead.
		> In model_train>traintest (line 127)
		  In model_train (line 55)
		  In run_experiments (line 127)
		Error using vl_svmtrain
		DATASET is not a structure.

		Error in model_train>traintest (line 133)
			  [w{c},b{c}] = vl_svmtrain(psi(:,train & y ~= 0), y(train & y ~= 0), 1/(n* C), ...

		Error in model_train (line 55)
		  info = traintest(opts, imdb, psi) ;

		Error in run_experiments (line 127)
				model_train(...

10. It seems clear that the problem is caused by `cat` a 23 GByte data struct `code`. The problem can be solved either by avoiding this `cat` process or adding more DDRAM devices (or equally, decreasing the data precision). The lattter can not always be possible. It is possible, however, to avoid the `cat` process by carefully rewriting this code block:

    ```[matlab]
    % model_train.m

    nn = size(batchResults{1,1}.code{1,1});
    code = zeros([numel(imageIds)*ts*nn, 1], 'single');
    for b = 1: numel(batches) % for each batch, `185 * 64` by default
        m = numel(batches{b}); % number of elements in the batch `batches{b}`
        for j = 1: m % for each element in a batch
            k = batches{b}(j); % get the ID of the element, which is in fact b*64+j
            for aa = 1:ts
                code(((k-1)*ts+aa) * nn : ((k-1)*ts+aa + 1) * nn) = batchResults{b}.code{(j-1)*ts+aa}(:);
            end
        end
    end
    ```
    For example, it can be rewrited as:

    ```[matlab]
    % model_train.m

    nn = size(batchResults{1,1}.code{1,1}, 1); % nn = 262144
    code = zeros([numel(imageIds)*ts*nn, 1], 'single'); % numel(imageIds) = 11788
    for b = 1: numel(batches) % for each batch, `185 * 64` by default
        m = numel(batches{b}); % number of elements in the batch `batches{b}`
        for j = 1: m % for each element in a batch
            k = batches{b}(j); % get the ID of the element, which is in fact b*64+j
            for aa = 1:ts
                code(((k-1)*ts+aa) * nn : ((k-1)*ts+aa + 1) * nn) = batchResults{b}.code{(j-1)*ts+aa}(:);
            end
        end
    end
    ```

    Holding this modification, try run `run_experiments` again ...
	...
	    Error using zeros
		Out of memory. Type HELP MEMORY for your options.

		Error in model_train>encoder_extract_for_images (line 248)
		code = zeros([numel(imageIds)*ts*nn, 1], 'single')

	Auch... It didn't work!

**OK, Seems to reach a deadend... Going to give up training a SVM...**

## Applying the B-CNN model with `softmax` layer (multiclass logistic regression)

After fine-tuning performed by `run_experiments_bcnn_train.m`, a fine-tuned model is trained and saved as `path/to/bcnn/checkgpu_mm/cub-seed-01/fine-tuned-model/final-model.mat`. Let's see what the saved model looks like.

    >> nett = load('data/checkgpu_mm/cub-seed-01/fine-tuned-model/final-model.mat')

    nett = 

        info: [1x1 struct]
         net: [1x1 struct]

    >> nett.info

    ans = 

        train: [1x1 struct]
          val: [1x1 struct]

    >> nett.net

    ans = 

        layers: {1x17 cell}
          meta: [1x1 struct]

    >> nett.info.train

    ans = 

            speed: [1x100 single]
        objective: [1x100 single]
            error: [2x100 single]

    >> nett.net.meta

    ans = 

               inputs: [1x1 struct]
              classes: [1x1 struct]
        normalization: [1x1 struct]

    >> nett.net.layers{17}

    ans = 

        type: 'l2norm'
        name: 'l2_norm'


Ah...Where is the softmax layer?! Without the softmax layer, it is just a bcnn feature extractor with no ability of classifying. To know what is going on, look into the `saveNetwork` function:

```[matlab]
% /path/to/bcnn/imdb_bcnn_train_dag.m

% -------------------------------------------------------------------------
function saveNetwork(fileName, net, info)
% -------------------------------------------------------------------------

% 
% % Replace the last layer with softmax
% layers{end}.type = 'softmax';
% layers{end}.name = 'prob';
net.layers(end-1:end) = [];

% Remove fields corresponding to training parameters
ignoreFields = {'learningRate',...
                'weightDecay',..
                'class'};
for i = 1:length(net.layers),
    net.layers{i} = rmfield(net.layers{i}, ignoreFields(isfield(net.layers{i}, ignoreFields)));
end

save(fileName, 'net', 'info', '-v7.3');
```
Obviously, the last two layers has been removed by `savedNetwork` function. To save the whole network, make the following change to the function:

```[matlab]
% /path/to/bcnn/imdb_bcnn_train_dag.m

% -------------------------------------------------------------------------
function saveNetwork(fileName, net, info)
% -------------------------------------------------------------------------

% 
% % Replace the last layer with softmax
net.layers{end}.type = 'softmax';
net.layers{end}.name = 'prob';
% net.layers(end-1:end) = [];
```
Then, run `run_experiments_bcnn_train.m` again.

